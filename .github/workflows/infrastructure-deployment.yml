name: Infrastructure Deployment & Management

on:
  push:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - 'kubernetes/**'
      - 'monitoring/**'
      - 'logging/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - 'kubernetes/**'
      - 'monitoring/**'
      - 'logging/**'
  workflow_dispatch:
    inputs:
      action:
        description: 'Infrastructure action'
        required: true
        default: 'plan'
        type: choice
        options:
        - plan
        - apply
        - destroy
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production

env:
  TF_VERSION: '1.6.0'
  AWS_REGION: 'me-south-1'
  KUBE_CONFIG_PATH: ~/.kube/config

jobs:
  # Terraform Infrastructure Management
  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'plan')
    outputs:
      plan-output: ${{ steps.plan.outputs.stdout }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        run: |
          cd terraform
          terraform init -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
                        -backend-config="key=reyada-homecare/terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}"

      - name: Terraform Validate
        run: |
          cd terraform
          terraform validate

      - name: Terraform Format Check
        run: |
          cd terraform
          terraform fmt -check

      - name: Terraform Plan
        id: plan
        run: |
          cd terraform
          terraform plan -var-file="environments/${{ github.event.inputs.environment || 'staging' }}.tfvars" \
                        -out=tfplan \
                        -detailed-exitcode
        continue-on-error: true

      - name: Comment Terraform Plan on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const output = `## Terraform Plan Results
            
            #### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
            #### Terraform Validation ü§ñ\`${{ steps.validate.outcome }}\`
            #### Terraform Format and Style üñå\`${{ steps.fmt.outcome }}\`
            #### Terraform Plan üìñ\`${{ steps.plan.outcome }}\`
            
            <details><summary>Show Plan</summary>
            
            \`\`\`terraform
            ${{ steps.plan.outputs.stdout }}
            \`\`\`
            
            </details>
            
            *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            });

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan
          path: terraform/tfplan

  terraform-apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: [terraform-plan]
    if: github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply')
    environment:
      name: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan
          path: terraform/

      - name: Terraform Init
        run: |
          cd terraform
          terraform init -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
                        -backend-config="key=reyada-homecare/terraform.tfstate" \
                        -backend-config="region=${{ env.AWS_REGION }}"

      - name: Terraform Apply
        run: |
          cd terraform
          terraform apply -auto-approve tfplan

      - name: Extract Terraform Outputs
        id: terraform-outputs
        run: |
          cd terraform
          echo "cluster_endpoint=$(terraform output -raw cluster_endpoint)" >> $GITHUB_OUTPUT
          echo "cluster_name=$(terraform output -raw cluster_name)" >> $GITHUB_OUTPUT
          echo "vpc_id=$(terraform output -raw vpc_id)" >> $GITHUB_OUTPUT

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ steps.terraform-outputs.outputs.cluster_name }}

      - name: Verify cluster connectivity
        run: |
          kubectl cluster-info
          kubectl get nodes

  # Kubernetes Infrastructure Deployment
  kubernetes-deployment:
    name: Deploy Kubernetes Infrastructure
    runs-on: ubuntu-latest
    needs: [terraform-apply]
    if: success() && (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name reyada-${{ github.event.inputs.environment || 'production' }}-cluster

      - name: Create namespace
        run: |
          kubectl create namespace reyada-homecare --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy application infrastructure
        run: |
          kubectl apply -f kubernetes/frontend-deployment.yaml
          kubectl apply -f kubernetes/database-deployment.yaml
          kubectl apply -f kubernetes/messaging-deployment.yaml
          kubectl apply -f kubernetes/api-gateway-deployment.yaml

      - name: Deploy blue-green infrastructure
        run: |
          kubectl apply -f kubernetes/blue-green-deployment.yaml

      - name: Deploy HPA and scaling policies
        run: |
          kubectl apply -f kubernetes/hpa-scaling.yaml

      - name: Verify deployments
        run: |
          kubectl get deployments -n reyada-homecare
          kubectl get services -n reyada-homecare
          kubectl get hpa -n reyada-homecare

  # Monitoring Infrastructure Deployment
  monitoring-deployment:
    name: Deploy Monitoring Infrastructure
    runs-on: ubuntu-latest
    needs: [kubernetes-deployment]
    if: success()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name reyada-${{ github.event.inputs.environment || 'production' }}-cluster

      - name: Deploy Prometheus monitoring
        run: |
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -f monitoring/prometheus/prometheus-config.yaml
          kubectl apply -f monitoring/prometheus/healthcare-alerts.yaml

      - name: Deploy Grafana dashboards
        run: |
          kubectl apply -f monitoring/grafana/healthcare-dashboards.yaml

      - name: Verify monitoring deployment
        run: |
          kubectl get pods -n monitoring
          kubectl get services -n monitoring

  # Logging Infrastructure Deployment
  logging-deployment:
    name: Deploy Logging Infrastructure
    runs-on: ubuntu-latest
    needs: [kubernetes-deployment]
    if: success()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name reyada-${{ github.event.inputs.environment || 'production' }}-cluster

      - name: Deploy Elasticsearch
        run: |
          kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -f logging/elasticsearch/elasticsearch-deployment.yaml

      - name: Wait for Elasticsearch to be ready
        run: |
          kubectl wait --for=condition=ready pod -l app=elasticsearch -n logging --timeout=600s

      - name: Deploy Kibana
        run: |
          kubectl apply -f logging/kibana/kibana-deployment.yaml

      - name: Deploy Fluentd log collectors
        run: |
          kubectl apply -f logging/fluentd/fluentd-daemonset.yaml

      - name: Verify logging deployment
        run: |
          kubectl get pods -n logging
          kubectl get services -n logging

  # Infrastructure Health Check
  infrastructure-health-check:
    name: Infrastructure Health Check
    runs-on: ubuntu-latest
    needs: [monitoring-deployment, logging-deployment]
    if: success()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name reyada-${{ github.event.inputs.environment || 'production' }}-cluster

      - name: Check cluster health
        run: |
          kubectl cluster-info
          kubectl get nodes
          kubectl get pods --all-namespaces

      - name: Check application health
        run: |
          kubectl get deployments -n reyada-homecare
          kubectl get services -n reyada-homecare
          kubectl get ingress -n reyada-homecare

      - name: Check monitoring health
        run: |
          kubectl get pods -n monitoring
          kubectl port-forward -n monitoring svc/prometheus 9090:9090 &
          sleep 10
          curl -f http://localhost:9090/-/healthy || echo "Prometheus health check failed"

      - name: Check logging health
        run: |
          kubectl get pods -n logging
          kubectl port-forward -n logging svc/elasticsearch-client 9200:9200 &
          sleep 10
          curl -f http://localhost:9200/_cluster/health || echo "Elasticsearch health check failed"

      - name: Run infrastructure tests
        run: |
          npm ci
          npm run test:infrastructure

      - name: Generate infrastructure report
        run: |
          npm run generate:infrastructure-report

      - name: Upload infrastructure report
        uses: actions/upload-artifact@v4
        with:
          name: infrastructure-report
          path: |
            infrastructure-report.html
            infrastructure-health.json

  # Disaster Recovery Test
  disaster-recovery-test:
    name: Disaster Recovery Test
    runs-on: ubuntu-latest
    needs: [infrastructure-health-check]
    if: success() && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name reyada-production-cluster

      - name: Test backup procedures
        run: |
          echo "Testing backup procedures..."
          kubectl create job --from=cronjob/database-backup manual-backup-test -n reyada-homecare
          kubectl wait --for=condition=complete job/manual-backup-test -n reyada-homecare --timeout=300s

      - name: Test failover procedures
        run: |
          echo "Testing failover procedures..."
          # Simulate failover by scaling down primary deployment
          kubectl scale deployment reyada-frontend-blue --replicas=0 -n reyada-homecare
          sleep 30
          # Verify green deployment takes over
          kubectl get pods -n reyada-homecare -l environment=green
          # Restore primary deployment
          kubectl scale deployment reyada-frontend-blue --replicas=3 -n reyada-homecare

      - name: Test monitoring alerting
        run: |
          echo "Testing monitoring alerting..."
          # Trigger test alert
          kubectl patch deployment reyada-frontend-blue -n reyada-homecare -p '{"spec":{"template":{"metadata":{"annotations":{"test-alert":"true"}}}}}'
          sleep 60
          # Reset
          kubectl patch deployment reyada-frontend-blue -n reyada-homecare -p '{"spec":{"template":{"metadata":{"annotations":{"test-alert":null}}}}}'

      - name: Generate DR test report
        run: |
          npm run generate:dr-test-report

      - name: Upload DR test report
        uses: actions/upload-artifact@v4
        with:
          name: disaster-recovery-test-report
          path: |
            dr-test-report.html
            dr-test-results.json

  # Notification
  notify-infrastructure-deployment:
    name: Notify Infrastructure Deployment
    runs-on: ubuntu-latest
    needs: [disaster-recovery-test]
    if: always()
    steps:
      - name: Notify deployment status
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#infrastructure'
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
          custom_payload: |
            {
              "text": "Infrastructure deployment completed",
              "attachments": [{
                "color": "${{ job.status == 'success' && 'good' || 'danger' }}",
                "fields": [
                  {"title": "Environment", "value": "${{ github.event.inputs.environment || 'production' }}", "short": true},
                  {"title": "Action", "value": "${{ github.event.inputs.action || 'apply' }}", "short": true},
                  {"title": "Status", "value": "${{ job.status }}", "short": true},
                  {"title": "Run ID", "value": "${{ github.run_id }}", "short": true}
                ]
              }]
            }
